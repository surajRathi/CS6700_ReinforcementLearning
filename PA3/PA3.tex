%! Author = suraj
%! Date = 3/27/23

% Preamble
\documentclass[A4]{article}
\usepackage[a4paper,left=2.3cm,right=2.3cm,top=2.3cm,bottom=2.3cm]{geometry}

% Packages
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage[outputdir=../out]{minted}
\usepackage{textcomp}
\usepackage{graphicx}

\title{Programming Assignment 3: Hierarchical Reinforcement Learning}
\date{Jan-May 2023}
\author{Suraj Rathi\\ ME19B177
\and Mrityunjay Upadhyay \\ OE22S002}

\begin{document}
    \maketitle
    \hrule


    \section{SMDP Q-Learning}

    \subsection{Description}

    \noindent We implemented SMDP Q-Learning for the \emph{Gymnasium Gymâ€™s Taxi-v3} environment.

    \subsubsection{Color Options SMDP}
    \begin{itemize}
        \item A \emph{Color} Option is one that can use the \{Move North, Move East, Move South, Move West\} options to reach a square of the required color.
        \item A \emph{Primitive} Option is one that can do a single action and then immediately terminates.
        \item The SMDP has a \emph{Color} option available for each of the colors along with one \emph{Primitive} option for `Pickup Passenger` and `Drop Passenger`
        \item With the limited option set available to it, we can now hide away a large amount of the states for the SMDP. We only expose the following
        \begin{enumerate}
            \item Which color the taxi is currently at or `other`.
            \item The location of the passenger
            \item The destination
        \end{enumerate}
    \end{itemize}

    \subsubsection{Color Option}
    \noindent This aims to make the taxi reach the square of a specified color.
    \begin{itemize}
        \item The option is assigned a single color.
        \item \emph{Initiation:} The option can be initiated at any state.
        There is no reason to choose that option when the taxi is already at that color, but in practice such an exclusion was automatically learned by the policy
        \item \emph{Policy:} The option contains its own Q-network.
        \begin{itemize}
            \item The network stores state as just the (taxi\_row, taxi\_col).
            \item The network can select an action from \{Move North, Move East, Move South, Move West\}.
            \item The network receives a reward as follows:
            \[
                \text{reward}= \left\{
                \begin{array}{ll}
                    10 & \text{if the taxi is at the respective colored square} \\
                    -1 & \text{otherwise}                                       \\
                \end{array}
                \right.
            \]
        \end{itemize}
        \item \emph{Termination:} the option terminates if the respective colored square is reached.
    \end{itemize}

    \subsubsection{Primitive Option}
    \noindent This is a dummy wrapper for a single action.
    \begin{itemize}
        \item The option is assigned a single action.
        \item \emph{Initiation:} The option can be initiated at any state.
        \item \emph{Policy:} The network can only choose the assigned action.
        \item \emph{Termination:} The option terminates at every state (i.e.\ it immediately terminates).
    \end{itemize}

    \subsubsection{Test Framework}
    Average reward is often a very satisfactory metric, but here we felt that it is more important to also track the fraction of times the environment is successfully solved.
    We run the network in evaluation (non-training) mode for 100 iterations with a set seed.

    \subsection{Implementation}
    We will focus on the \mintinline{python}{policy} and \mintinline{python}{update} functions for the Color Option SMDP and Color option.
    In the following snippets, if the eval (short for evaluation) options is set, the network does not modify its internal state (i.e.\ it does not learn).
    All the SMDP networks and runners run off a single seeded generator, hence the results are completely reproducible.

    \subsubsection{Color Options SMDP}
    \begin{minted}[linenos]{python}
        def policy(self, state: int, eval=False) -> int:
            if self.option is None:
                low_state = self.to_lower_state(state)

                self.option_id = self.net.policy(low_state, eval=eval)
                self.option = self.options[self.option_id]
                self.option.start()
                self.l_start_state = low_state
                self.r_bar = 0.0
                self.tau = 0

            return self.option.policy(state, eval=eval)
    \end{minted}

    If no option is selected, we reduce our state's dimensionality and sample from the Q-network to choose one.
    We then execute the currently selected option's policy.

    \begin{minted}[linenos]{python}
        def update(self, state: int, action: int, reward: float, next_state: int, done: bool, eval=False) -> None:
            self.option.update(state, action, reward, next_state, done, eval)

            self.r_bar += (self.gamma ** self.tau) * reward
            self.tau += 1
            if self.option.is_done():
                self.option.finish()
                self.option = None

                l_next_state = self.to_lower_state(next_state)
                self.net.update(
                    self.l_start_state, self.option_id, self.r_bar, l_next_state, done, eval, tau=self.tau
                )
    \end{minted}

    We first execute the option's update function.
    If the option is complete, we use the q-learning update equation to update our SMDP's policy.

    \subsubsection{Color Option}
    \begin{minted}[linenos]{python}
        def policy(self, state, eval=False):
            state = self.to_lower_state(state)
            action = self.net.policy(state, eval=eval)
            return self.to_higher_action(action)


        def update(self, state, action, reward, next_state, done, eval=False):
            state = self.to_lower_state(state)
            action = self.to_lower_action(action)
            next_state = self.to_lower_state(next_state)

            reward = -1
            if next_state == self.terminal:
                self._done = True
                reward = 10

            self.net.update(state, action, reward, next_state, done, eval)
    \end{minted}

    The above snippet is quite similar to the Color Options SMDP's code.
    As before, the main operations are reducing the dimensionality fo the state and actions.
    We should note the custom reward function on lines 12 to 15.

    \subsection{Performanance}
    The network was trained for 3000 episodes with relatively standard hyperparameter values.
    In the below figure we can see the test results and reward curve for the network.
    To offer a baseline for reference, we show the results of vanilla Q-Learning
    as well as Color Options SMDP where the outer policy was set by a human expert instead of learned.

    The yellow line indicates the performance of the Color Option SMDP network.

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{./img/smdp_performance_comparision}
        \caption{Reward curves and test performance for the Color Options SMDP Q-Learning Network.}
        \label{fig:smdp_performance_comparision}
    \end{figure}

    We see that the option based framework converges much quicker and shows a higher average reward over the training process.
    The Q-learning based approach takes at least twice the number of episodes to show similar average reward
    and 2.5 times the episodes to show similar test success rates.

    \subsection{Policy Visualization and Description}

    \subsubsection{SMDP Policy}
    \begin{figure}[H]
        \centering
        \includegraphics[width= 0.7\textwidth]{./img/smdp_policy_labelled}
        \caption{A matrix showing the SMDP's policy and value functions.}\label{fig:smdp_policy}
    \end{figure}

    In the above figure, we can see a heatmap of the value function of the states in the SMDP's Q-network.
    The action with the highest value is written in the square.
    Darker colors indicate higher value.
    We can summarize the policy in two lines:
    \begin{enumerate}
        \item Go to the color where the passenger is in, and execute the pickup action.
        \item Go to the color of the destination, and execute the dropoff action.
    \end{enumerate}

    This is reasonable.
    Our goal is to driop the passenger off at the destination in as few steps as possible (reduces the negative reward).
    The above policy achieves that goal.

    \subsubsection{Color Option Policy}
    The goal of the color option policy is to bring the taxi to the required colored square in as few steps as possible.
    Below we can see the value function for each of the color options as a heatmap and the policy superimposed in the form of arrows.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7 \textwidth]{./img/smdp_option_policy}
        \caption{A representation of the policy of the color-options.}\label{fig:smdp_option_policy}
    \end{figure}

    The policy is reasonable as it assigns squares closer to the goal color with higher value
    and the arrows point in the direction of the respective colored square.

    \subsection{Pick-Drop Option Formulation}
    We framed a set of options which are mutually exclusive with the given options, we refer to this as the `Pick-Drop Formulation`.

    \subsubsection{Description}

    \subsubsection{PickDrop SMDP}
    \begin{itemize}
        \item The SMDP has the following options available: \{Pick, Drop\}
        \item The SMDP has just two states: \{Passenger not in Taxi, Passenger in Taxi\}
    \end{itemize}

    \subsubsection{Pick Option}
    \begin{itemize}
        \item \emph{Initiation:} The option can be initiated at any state.
        \item \emph{Policy:} The option contains its own Q-network.
        \begin{itemize}
            \item The network stores state as just the (taxi\_row, taxi\_col, passenger\_color).
            \item The network can select an action from \{Move North, Move East, Move South, Move West, Pickup Passenger\}.
            \item The network receives a reward as follows:
            \[
                \text{reward}= \left\{
                \begin{array}{ll}
                    10 & \text{if the passenger is in the taxi} \\
                    -1 & \text{otherwise}                       \\
                \end{array}
                \right.
            \]
        \end{itemize}
        \item \emph{Termination:} the option terminates when the passenger is in the taxi
    \end{itemize}

    \subsubsection{Drop Option}
    \begin{itemize}
        \item \emph{Initiation:} The option can be initiated at any state.
        \item \emph{Policy:} The option contains its own Q-network.
        \begin{itemize}
            \item The network stores state as just the (taxi\_row, taxi\_col, destination\_color).
            \item The network can select an action from \{Move North, Move East, Move South, Move West, Drop Passenger\}.
            \item The network receives the same reward as the SMDP
        \end{itemize}
        \item \emph{Termination:} the option terminates when the passenger is no longer in the taxi
    \end{itemize}

    \subsubsection{Results}
    The SMDP learned the following value function.
    We can see that it tries to pick up the passenger if he isn't currently in the taxi, and tries to drop him off otherwise.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.3 \textwidth]{./img/pick_drop_value_func}
        \caption{PickDrop SMDP Value Function}
        \label{fig:pick_drop_value_func}
    \end{figure}

    \noindent We ran three option networks to compare the performance: Primitive Option SMDP, Color Options SMDP, and PickDrop Options SMDP.
    The Primitive option network performs identically to the vanilla Q-Learning approach.
    It just wraps each of the basic actions with an immediately terminating option.
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{./img/smdp_option_comparision}
        \caption{Comparision of different Option formulations.}
        \label{fig:smdp_option_comparision}
    \end{figure}

    We can see that the Pick-Drop option formulation performs better than the given (Color) option formulation.
    It is quicker to converge and shows a higher average reward.


    \section{Intra-Option Q-Learning}


    \section{Conclusion: SMDP Q-Learning vs Intra-Option Q-Learning}
\end{document}