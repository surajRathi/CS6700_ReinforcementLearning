%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[addpoints,12pt,solution]{exam}
\printanswers
\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{paralist}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{marvosym}
\DeclareMathOperator*{\argmax}{argmax\,}
\DeclareMathOperator*{\argmin}{argmin\,}
\DeclareMathOperator*{\real}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
\usepackage{enumitem}
\usepackage{amsfonts}
\marksnotpoints


\begin{document}


    \hrule
    \vspace{1mm}
    \noindent
    \begin{center}
    {\Large CS6700 : Reinforcement Learning}
        \\
        {\large Written Assignment \#1}
    \end{center}
    \vspace{1mm}
    \noindent
    {\textbf{Topics}: Intro, Bandits, MDP, Q-learning, SARSA, PG \hfill \textbf{Deadline}: 20 March 2023, 23:55}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Enter name and roll number here
    \noindent {\bf Name: } Suraj Rathi  \hfill {\bf Roll number: } ME19B177
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \vspace{2mm}
    \hrule

    {\small

        \begin{itemize}
            \itemsep0mm
            \item This is an individual assignment. Collaborations and discussions are strictly
            prohibited.
            \item Be precise with your explanations. Unnecessary verbosity will be penalized.
            \item Check the Moodle discussion forums regularly for updates regarding the assignment.
            \item Type your solutions in the provided \LaTeX template file.
            \item \textbf{Please start early.}
        \end{itemize}
    }

    \hrule

    \vspace{3mm}


%\gradetable[h][questions]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%START HERE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{questions}

        \question[2][Bandit Question] Consider a N-armed slot machine task, where the rewards for
        each arm $a_i$ are usually generated by a stationary distribution with mean $Q^{*}(a_i)$. The machine is under repair when a arm is pulled, a small fraction, $\epsilon$, of the times a random arm is activated. What is the expected payoff for pulling arm $a_i$ in this faulty machine?

        \begin{solution}
            \begin{align*}
                q^*(a_i) & = (1-\epsilon) Q^*(a_i) + \epsilon \frac{1}{N} \sum_{j = 1}^{N} {Q^*(a_j)} \\
                & = Q^*(a_i) +  \epsilon \frac{1}{N} \sum_{j = 1}^{N} {(Q^*(a_j)-Q^*(a_i))}
            \end{align*}
        \end{solution}


        \question[4][Delayed reward] Consider the task of controlling a system when the control actions are delayed. The control agent takes an action on observing the state at time $t$. The action is applied to the system at time $t + \tau$. The agent
        receives a reward at each time step.
        \begin{enumerate}[label=(\alph*)]

            \item (2 marks)What is an appropriate notion of return for this task?

            \begin{solution}

                In the standard case:
                \[G_t = \sum_{k=0}^{\infty} \gamma^{k}R_{t + k + 1}\]

                As the action is applied to the system at time $t + \tau$, only the rewards received after time $t + \tau$ are relevant.
                Thus, we can define return as:

                \[G_t = \sum_{k=0}^{\infty} \gamma^{k}R_{t + \tau + k + 1}\]

            \end{solution}

            \item (2 marks) Give the TD(0) backup equation for estimating the value function of a given policy.

            \begin{solution}

                The Monte-Carlo update equation is as follows:

                \[V(S_t) \leftarrow (1-\alpha) * V(S_t) + \alpha * G_t\]

                In our case:
                \begin{align*}
                    V(S_t) &\leftarrow (1-\alpha) * V(S_t) + \alpha * ( \sum_{k=0}^{\infty} \gamma^{k}R_{t + \tau + k + 1})\\
                    &\leftarrow (1-\alpha) * V(S_t) + \alpha * (R_{t + \tau + k + 1} +  \sum_{k=1}^{\infty} \gamma^{k}R_{t + \tau + k + 1})\\
                    &\leftarrow (1-\alpha) * V(S_t) + \alpha * (R_{t + \tau + k + 1} + V(S_{t + 1}))
                \end{align*}

                The final $TD(0)$ update equation:
                \[
                    V(S_t) \leftarrow V(S_t) + \alpha * (R_{t + \tau + k + 1} + V(S_{t + 1}) - V(S_t))
                \]


            \end{solution}
        \end{enumerate}

        \question[5][Reward Shaping] Consider two finite MDPs $M_1,\ M_2$ having the same state set, $S$, the same action set, $A$, and respective optimal action-value functions $Q^*_1,\ Q^*_2$. (For simplicity, assume all actions are possible in all states.) Suppose that the following is true for an arbitrary function $f: S \rightarrow R$ :\\
        \begin{center}
            $Q^*_2(s, a) = Q^*_1(s, a) - f(s)$
        \end{center}
        for all $s \in S$ and $a \in A$.
        \begin{enumerate}[label=(\alph*)]
            \item (2 marks) Show mathematically that $M_1$ and $M_2$ has same optimal policies.
            \begin{solution}
                \[
                    \pi^*_1(s) = \argmax_{a\in \mathcal{A}} Q^*_1(s,a)
                \]
                For $M_2$, as $f$ does not depend on the action:
                \begin{align*}
                    \pi^*_2(s) &= \argmax_{a\in \mathcal{A}} Q^*_2(s,a)\\
                    &= \argmax_{a\in \mathcal{A}}(Q^*_1(s,a) - f(s))\\
                    &= \argmax_{a\in \mathcal{A}}(Q^*_1(s,a))
                \end{align*}

                Hence we see that $\pi^*_1(s)$ and $\pi^*_2(s)$ are equivalent.

            \end{solution}

            \item (3 marks) Now assume that $M_1$ and $M_2$ has the same state transition probabilities but different reward functions. Let $R_1(s, a, s')$ and $R_2(s, a, s')$ give the expected immediate reward for the transition from $s$ to $s'$ under action $a$ in $M_1$ and $M_2$, respectively. Given the optimal state-action value functions are related as given above, what is the relationship between the functions $R_1$ and $R_2$ ? That is, what is $R_1$ in terms of $R_2$ and $f$; OR $R_2$ in terms of $R_1$ and $f$.

            \begin{solution}
                Using the Bellman optimality condition,

                \begin{align*}
                    Q^*(s, a) &= \E_{\pi^*}\left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t=s, A_t =a \right]\\
                    &= \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma \max _{a^{\prime}} Q^*\left(s^{\prime}, a^{\prime}\right)\right]\\
                \end{align*}
                We can apply that to $M_1$ and $M_2$:
                % Sutton 3.20\\
                \begin{align*}
                    Q^*_2(s, a) &=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[R_2(s, a, s^\prime)+\gamma \max _{a^{\prime}} Q^*_2\left(s^{\prime}, a^{\prime}\right)\right] \\
                    Q^*_1(s, a) - f(s) &=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[R_2(s, a, s^\prime)+\gamma \max _{a^{\prime}} (Q^*_1\left(s^{\prime}, a^{\prime}\right) - f(s^\prime))\right] \\
                    Q^*_1(s, a) &=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[\left(R_2(s, a, s^\prime) + f(s) - \gamma f(s^\prime)\right) +\gamma \max _{a^{\prime}} (Q^*_1\left(s^{\prime}, a^{\prime}\right))\right] \\
                \end{align*}

                We know that
                \[
                    Q^*_1(s, a) =\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[R_1(s, a, s^\prime)+\gamma \max _{a^{\prime}} Q^*_1\left(s^{\prime}, a^{\prime}\right)\right]
                \]

                Comparing the two equations:
                \[
                    R_1(s, a, s^\prime) = R_2(s, a, s^\prime) + f(s) - \gamma f(s^\prime)
                \]
            \end{solution}

        \end{enumerate}



        \question[10][Jack's Car Rental] Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited \$ 10 by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of \$ 2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number $n$ is $\frac{\lambda^n}{n!}e^{-\lambda}$, where $\lambda$ is the expected number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a
        maximum of five cars can be moved from one location to the other in one night.

        \begin{enumerate}[label=(\alph*)]

            \item (4 marks) Formulate this as an MDP. What are the state and action sets? What is the reward function? Describe the transition probabilities (you can use a formula rather than a tabulation of them, but be as explicit as you can about the probabilities.) Give a definition of return and describe why it makes sense.

            \begin{solution}
                % TODO: How does tuple notation work
                Let us define the problem $M = \langle \mathcal{S}, \mathcal{A}, \mathcal{A}(s), p, r, \gamma \rangle$
                \begin{align*}
                    \mathcal{K} &= \{ 0, 1, 2 \dots , 20\} \\
                    \mathcal{S} &= K^2 \\
                    \mathcal{A} &= \{-5, -4, \dots, 4, 5\} \\
                \end{align*}
                The state is the number of cars at each location. For a state $s \in \mathcal{S}$,
                $s_1$ refers to the number of cars at the first location,
                and $s_2$ refers to the number of cars at the second location.
                The action is the number of cars moved from the first to the second location,
                where negative numbers represent movement in the other direction.

                \[
                    \mathcal{A}(s) = \{a : a \in \mathcal{A}, a \leq s_1, a \geq -s_2, s_1 - a \leq 20, s_2 + a \leq 20\}, s \in \mathcal{S}
                \]

                Then we shall define the random variables determining the state transition at each timestep:
                \begin{align*}
                    N_{rent}^{(1)} &\sim \operatorname{Pois}(3) \\
                    N_{rent}^{(2)} &\sim \operatorname{Pois}(4) \\
                    N_{return}^{(1)} &\sim \operatorname{Pois}(3) \\
                    N_{return}^{(2)} &\sim \operatorname{Pois}(2) \\
                \end{align*}

                For a state $s \in \mathcal{S}$ and an action $a \in \mathcal{A}$, we can define the next state, $\textbf{s'}$,
                as a 2-tuple of random variables and reward, $\textbf{r}$, as random variables.
                \begin{align*}
                    k &= s + \{-a, a\} \\
                    N_{rented} &= \{\min{(k_i, N_{rent}^{(i)})} : i \in {1,2} \} \\
                    k &= k - N_{rented} \\
                    n_{rented} &= \{\min{(20, k_i + N_{return}^{(i)})} : i \in {1,2} \} \\
                    \textbf{s'} &= \{k_1, k_2\}\\
                    \textbf{r} &= 2 * |a| + 10 *( N_{rented}^{(1)} + N_{rented}^{(2)})
                \end{align*}
                With those random variables defined, we can write down the probability distribution as follows:

                \begin{align*}
                    p(s'| s, a) &= \mathbb{P}[\textbf{s'} = s'] \\
                    p(r| s, a) &= \mathbb{P}[\textbf{r} =r ] \\
                    r &= \E[\textbf{r}]
                \end{align*}

                In the given task, we will use the notion of discounted return.
                \[
                    G_t = \sum_{k=0}^{\infty}\gamma^k r_{t+k+1}
                \]

                This is reasonable, as in the physical sense, the reward is money.
                This means that future rewards are worth less than immediate rewards.
                In addition, the problem has an infinite time horizon, so to handle that we must use the concept of discounted returns.
                % TODO: Elaborate
                For the given problem, we could take $\gamma = 0.9$.


            \end{solution}

            \item (3 marks) One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \$ 2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of \$ 4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than
            dynamic programming. Can you think of a way to incrementally change your MDP formulation above to account for these changes?

            \begin{solution}
                Here we would need to modify the reward generating step.
                After the process described in 4.a, we must perform the following manipulation to the random variable.
                \[
                    \textbf{r} = \textbf{r} + 2 * H(a) - 4 * H(k_1 - 10) - 4 * H(k_2 - 10)
                \]
                Here, $H(x)$ refers to the Heaviside step function.
            \end{solution}

            \item (3 marks) Describe how the task of Jack’s Car Rental could be reformulated in terms of \textit{afterstates}. Why, in terms of this specific task, would such a reformulation be likely to speed convergence? \textit{(Hint:- Refer page 136-137 in RL book 2nd edition. You can also refer to the video at \href{https://www.youtube.com/watch?v=w3wGvwi336I}{https://www.youtube.com/watch?v=w3wGvwi336I})}

            \begin{solution}
                %  TODO: you can explain this much better.
                In the given question, we first move the cars and then apply a stochastic update based on the number of rentals and returns.
                There are multiple states, which after a given action, would lead to the same number of cars in each lot for the stochastic step.

                For example, let $s_1 = \{12, 10\}$) and $a_1 = 2$.
                After applying the action, we get a state $s_{1i} = \{10, 10 \}$.
                Similarly, for a state  $s_2 = \{10, 10\}$) and $a_1 = 0$, we get the same intermediate state $s_{2i} = \{10, 10 \}$

                With both of the intermediate states, we must apply the stochastic update to get the final new state.
                The state transition $s_{1i} \rightarrow s'_{1}$ is a possible realization of the state transition
                from  $s_{2i} \rightarrow s'_{2}$.
                However, our current formulation does not let each of the states learn from the other one.

                We can call this intermediate state the `after state' and apply that concept here.
                Here, instead of learning the action-value formulation, we can learn the afterstate-value formulation.
                This is helpful as
                \begin{enumerate}
                    \item We reduce the memory required.
                    The action value formulation has an upper bound of $20 * 20 * 10$ entries.
                    The afterstate-value formulation only requires $20 * 20$ entires.
                    This is an order of magnitude less than the first one.
                    The advantages are more apparent when you consider problems with larger action space.
                    \item Each afterstate gets more data from a fixed number of runs.
                    This will massively speed up convergence.
                \end{enumerate}


            \end{solution}

        \end{enumerate}

        \question[8][Organ Playing] You receive the following letter:\\
        Dear Friend, Some time ago, I bought this old house, but found it to be haunted by
        ghostly sardonic laughter. As a result it is hardly habitable. There is hope, however,
        for by actual testing I have found that this haunting is subject to certain laws, obscure
        but infallible, and that the laughter can be affected by my playing the organ or burning
        incense. In each minute, the laughter occurs or not, it shows no degree. What it will
        do during the ensuing minute depends, in the following exact way, on what has been
        happening during the preceding minute: Whenever there is laughter, it will continue in
        the succeeding minute unless I play the organ, in which case it will stop. But continuing
        to play the organ does not keep the house quiet. I notice, however, that whenever I
        burn incense when the house is quiet and do not play the organ it remains quiet for the
        next minute. At this minute of writing, the laughter is going on. Please tell me what
        manipulations of incense and organ I should make to get that house quiet, and to keep
        it so.\\
        Sincerely,\\
        At Wits End

        \begin{enumerate}[label=(\alph*)]

            \item (4 marks) Formulate this problem as an MDP (for the sake of uniformity, formulate it as a
            continuing discounted problem, with $\gamma= 0.9$. Let the reward be +1 on any transition
            into the silent state, and -1 on any transition into the laughing state.) Explicitly give the
            state set, action sets, state transition, and reward function.

            \begin{solution}
                The above problem is a deterministic MDP.

                Let us define the state space as: $\mathcal{S} = \{L, Q\}$,
                where $L$ indicates laughter and $Q$ indicates quiet.

                If we let $I$ indicate burning incense and $O$ as playing the organ, the action space is $\mathcal{A} = \{I, O\}$

                The state transition and reward function are shown in a tabular form below.

                \begin{center}
                    \begin{tabular}{|cc|cc|}
                        \hline
                        State & Action & New State & Reward \\
                        \hline
                        L     & I      & L         & -1     \\
                        L     & O      & Q         & 1      \\
                        Q     & I      & Q         & 1      \\
                        Q     & O      & L         & -1     \\
                        \hline
                    \end{tabular}
                \end{center}


            \end{solution}

            \item (2 marks) Starting with simple policy of \textbf{always} burning incense, and not playing organ, perform a couple of policy iterations.

            \begin{solution}

                \begin{enumerate}
                    \item \textbf{Initialization}

                    Initial policy: $\pi(L) = I, \pi(Q) = I$

                    Initial value: $V(L) = 0,V(Q) = 0$

                    \item \textbf{Policy Evaluation}
                    \begin{align*}
                        V(L) &= -1 + 0.9 * V(L)  = -1 \\
                        V(Q) &= 1 + 0.9 * V(Q) = 1 \\
                    \end{align*}
                    After repeating, many times:
                    \begin{align*}
                        V(L) &= -1 + 0.9 * -10  = -10 \\
                        V(Q) &= 1 + 0.9 * 10 = 10 \\
                    \end{align*}

                    \item \textbf{Policy Improvement}
                    \begin{align*}
                        \pi(L) &= O \\
                        \pi(Q) &= I \\
                    \end{align*}

                    \item \textbf{Policy Evaluation}
                    \begin{align*}
                        V(L) &= 1 + 0.9 * V(Q) = 10 \\
                        V(Q) &= 1 + 0.9 * V(Q) = 10 \\
                    \end{align*}


                    \item \textbf{Policy Improvement}
                    \begin{align*}
                        \pi(L) &= O \\
                        \pi(Q) &= I \\
                    \end{align*}

                \end{enumerate}
                We see that the policy is stable, and hence we have found the optimal value function and optimal policy.

            \end{solution}

            \item (2 marks) Finally, what is your advice to ``At Wits End"?

            \begin{solution}
                In the current minute, play the organ.
                After than, stop playing the organ and burn incense forever.
            \end{solution}


        \end{enumerate}

        \question[4][Stochastic Gridworld] An $\epsilon$-greedy version of a policy means that with probability 1-$\epsilon$ we follow the policy action and for the rest we uniformly pick an action.
        Design a stochastic gridworld where a deterministic policy will produce
        the same trajectories as a $\epsilon$-greedy policy in a deterministic
        gridworld. In other words, for every trajectory under the same policy, the
        probability of seeing it in each of the worlds is the same. By the same policy I mean that in the stochastic gridworld, you have a deterministic policy and in the
        deterministic gridworld, you use the same policy, except for $\epsilon$ fraction of the actions, which you choose uniformly randomly.

        \begin{enumerate}[label=(\alph*)]

            \item (2 marks) Give the complete specification of the world.

            \begin{solution}


            \end{solution}


            \item (2 marks) Will SARSA on the two worlds converge to the same policy? Justify.

            \begin{solution}

            \end{solution}

        \end{enumerate}

        \question[5][Contextual Bandits] Consider the standard multi class classification task (Here, the goal is to construct a function which, given a new data point, will correctly predict the class to which the new point belongs). Can we formulate this as contextual bandit problem (Multi armed Bandits with side information) instead of standard supervised learning setting? What are the pros/cons over the supervised learning method. Justify your answer. Also describe the complete Contextual Bandit formulation.

        \begin{solution}

        \end{solution}

        \question[5] [TD, MC, PG] Suppose that the system that you are trying to learn about (estimation or control) is not perfectly Markov. Comment on the suitability of using different solution approaches for such a task, namely, Temporal Difference learning, Monte Carlo methods, and Policy Gradient algorithms. Explicitly state any assumptions that you are making.
        \begin{solution}

        \end{solution}

        \question[5] [PG] Recent advances in computational learning theory, have led to the development of very powerful classification engines. One way to take advantage of these classifiers is to turn the reinforcement learning problem into a classification problem. Here the policy is treated as a labeling on the states and a suitable classifier is trained to learn the labels from a few samples. Once the policy is adequately represented, it can be then used in a policy evaluation stage. Can this method be considered a policy gradient method? Justify your answer. Describe a complete method that generates appropriate targets for the classifier.
        \begin{solution}

        \end{solution}

    \end{questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%THE END%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}